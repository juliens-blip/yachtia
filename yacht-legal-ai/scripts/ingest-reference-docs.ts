/**
 * Script d'ingestion automatique des documents de rÃ©fÃ©rence
 * 
 * Processus:
 * 1. TÃ©lÃ©charge PDFs et scrape pages HTML
 * 2. Extrait le texte
 * 3. Chunke le texte (500 tokens, 200 overlap)
 * 4. GÃ©nÃ¨re les embeddings (batch de 10)
 * 5. Stocke dans Supabase (documents + document_chunks)
 * 
 * Usage:
 *   npm run ingest:all          # IngÃ¨re tous les documents
 *   npm run ingest:category -- MYBA  # IngÃ¨re une catÃ©gorie
 */

// CRITICAL: Load env FIRST before any imports
import './load-env'

import { REFERENCE_DOCS, type ReferenceDocument, getReferenceStats } from './reference-urls'
import { extractTextFromPDF } from '../lib/pdf-parser'
import { scrapeWebPage, downloadPDF } from '../lib/web-scraper'
import { chunkText } from '../lib/chunker'
import { generateEmbedding } from '../lib/gemini'
import { supabaseAdmin } from '../lib/supabase'

// Configuration
const BATCH_SIZE = 10  // Embeddings par batch (rate limiting)
const DELAY_BETWEEN_BATCHES = 2000  // 2 secondes de dÃ©lai
const RETRY_ATTEMPTS = 3
const RETRY_DELAY = 5000  // 5 secondes

// Statistiques globales
let stats = {
  totalDocuments: 0,
  totalChunks: 0,
  totalErrors: 0,
  categoriesProcessed: new Set<string>(),
  startTime: Date.now()
}

/**
 * Sleep utility
 */
function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}

function sanitizeText(input: string): string {
  return input
    .replace(/\u0000/g, '')
    .replace(/[\uD800-\uDFFF]/g, '')
}

/**
 * Ingestion d'un seul document avec retry logic
 */
async function ingestDocument(
  doc: ReferenceDocument,
  category: string,
  retryCount = 0
): Promise<{ success: boolean; chunks: number }> {
  try {
    console.log(`\nðŸ“„ [${category}] ${doc.name}`)
    console.log(`   URL: ${doc.url}`)
    
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Ã‰TAPE 1: TÃ©lÃ©charger et extraire texte
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let text: string
    let pages: number | undefined
    
    if (doc.type === 'pdf') {
      const buffer = await downloadPDF(doc.url)
      const parsed = await extractTextFromPDF(buffer)
      text = parsed.text
      pages = parsed.pages
      console.log(`   ðŸ“– ${pages} pages extraites`)
    } else {
      text = await scrapeWebPage(doc.url)
      console.log(`   ðŸ“° Article web extrait`)
    }
    
    text = sanitizeText(text)

    if (!text || text.length < 100) {
      console.warn('   âš ï¸  Text too short, inserting placeholder')
      text = `${doc.name}\nSource: ${doc.url}\n[PDF content not extractable]`
    }
    
    console.log(`   âœ‚ï¸  Texte total: ${text.length} caractÃ¨res`)
    
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // ETAPE 2: Stocker document dans DB
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const { data: document, error: docError } = await supabaseAdmin
      .from('documents')
      .insert({
        name: doc.name,
        category,
        pages: pages || null,
        file_url: doc.url,
        source_url: doc.url,  // Also set source_url for compatibility
        is_public: true,      // Required for RAG search (RLS policy)
        metadata: {
          source: doc.url,
          type: doc.type,
          language: doc.language || 'en',
          ingested_at: new Date().toISOString()
        }
      })
      .select('id')
      .single()
    
    if (docError || !document) {
      throw new Error(`Failed to insert document: ${docError?.message || 'Unknown error'}`)
    }
    
    console.log(`   ðŸ’¾ Document ID: ${document.id}`)
    
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Ã‰TAPE 3: Chunker le texte
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const chunks = chunkText(text, 500, 200)
    console.log(`   âœ‚ï¸  ${chunks.length} chunks crÃ©Ã©s`)
    
    if (chunks.length === 0) {
      console.warn(`   âš ï¸  No chunks created (empty text?)`)
      return { success: true, chunks: 0 }
    }
    
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // ETAPE 4: Generer embeddings (batch processing)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const chunkRecords: Array<{
      document_id: string
      chunk_index: number
      chunk_text: string
      chunk_vector: number[]  // Correct column name from migration 003
      page_number: number | null
      token_count: number
    }> = []
    
    const totalBatches = Math.ceil(chunks.length / BATCH_SIZE)
    
    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batch = chunks.slice(i, i + BATCH_SIZE)
      const batchNumber = Math.floor(i / BATCH_SIZE) + 1
      
      console.log(`   ðŸ”¢ Batch ${batchNumber}/${totalBatches} (${batch.length} chunks)`)
      
      try {
        // Generate embeddings in parallel for this batch
        const embeddings = await Promise.all(
          batch.map(chunk => generateEmbedding(chunk.text))
        )

        // Build records
        batch.forEach((chunk, j) => {
          chunkRecords.push({
            document_id: document.id,
            chunk_index: i + j,
            chunk_text: chunk.text,
            chunk_vector: embeddings[j],  // Correct column name
            page_number: null,  // Could be inferred from chunk position if needed
            token_count: chunk.tokenCount
          })
        })
        
        console.log(`   âœ… Batch ${batchNumber} embeddings generated`)
        
        // Delay between batches (rate limiting)
        if (i + BATCH_SIZE < chunks.length) {
          await sleep(DELAY_BETWEEN_BATCHES)
        }
        
      } catch (error) {
        console.error(`   âŒ Batch ${batchNumber} failed:`, error instanceof Error ? error.message : error)
        throw new Error(`Embedding generation failed at batch ${batchNumber}`)
      }
    }
    
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Ã‰TAPE 5: InsÃ©rer chunks dans DB
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const { error: chunksError } = await supabaseAdmin
      .from('document_chunks')
      .insert(chunkRecords)
    
    if (chunksError) {
      throw new Error(`Failed to insert chunks: ${chunksError.message}`)
    }
    
    console.log(`   âœ… ${chunkRecords.length} chunks insÃ©rÃ©s dans Supabase`)
    console.log(`   â±ï¸  Temps: ${((Date.now() - stats.startTime) / 1000).toFixed(1)}s`)
    
    // Update stats
    stats.totalDocuments++
    stats.totalChunks += chunkRecords.length
    stats.categoriesProcessed.add(category)
    
    return { success: true, chunks: chunkRecords.length }
    
  } catch (error) {
    console.error(`   âŒ Error:`, error instanceof Error ? error.message : error)
    
    // Retry logic
    if (retryCount < RETRY_ATTEMPTS) {
      console.log(`   ðŸ”„ Retrying (${retryCount + 1}/${RETRY_ATTEMPTS}) in ${RETRY_DELAY / 1000}s...`)
      await sleep(RETRY_DELAY)
      return ingestDocument(doc, category, retryCount + 1)
    }
    
    stats.totalErrors++
    return { success: false, chunks: 0 }
  }
}

/**
 * Ingestion d'une catÃ©gorie complÃ¨te
 */
async function ingestCategory(categoryName: string) {
  const docs = REFERENCE_DOCS[categoryName]
  
  if (!docs) {
    console.error(`âŒ CatÃ©gorie inconnue: ${categoryName}`)
    console.log(`   CatÃ©gories disponibles: ${Object.keys(REFERENCE_DOCS).join(', ')}`)
    return
  }
  
  console.log(`\n${'â•'.repeat(60)}`)
  console.log(`ðŸ“ CATÃ‰GORIE: ${categoryName} (${docs.length} documents)`)
  console.log(`${'â•'.repeat(60)}`)
  
  for (let i = 0; i < docs.length; i++) {
    const doc = docs[i]
    console.log(`\n[${i + 1}/${docs.length}]`)
    await ingestDocument(doc, categoryName)
  }
}

/**
 * Ingestion de toutes les catÃ©gories
 */
async function ingestAll() {
  console.log('â•”' + 'â•'.repeat(58) + 'â•—')
  console.log('â•‘' + ' '.repeat(58) + 'â•‘')
  console.log('â•‘  ðŸš€ INGESTION AUTOMATIQUE DES DOCUMENTS DE RÃ‰FÃ‰RENCE  â•‘')
  console.log('â•‘' + ' '.repeat(58) + 'â•‘')
  console.log('â•š' + 'â•'.repeat(58) + 'â•\n')
  
  const docStats = getReferenceStats()
  console.log('ðŸ“Š Statistiques:')
  console.log(`   Total: ${docStats.totalDocuments} documents`)
  console.log(`   PDFs: ${docStats.totalPDFs}`)
  console.log(`   Pages HTML: ${docStats.totalHTML}`)
  console.log(`   CatÃ©gories: ${docStats.categories}`)
  console.log('\nðŸ“‹ Par catÃ©gorie:')
  for (const [cat, count] of Object.entries(docStats.breakdown)) {
    console.log(`   - ${cat}: ${count} documents`)
  }
  
  console.log('\nâ³ DÃ©but de l\'ingestion...\n')
  stats.startTime = Date.now()
  
  for (const categoryName of Object.keys(REFERENCE_DOCS)) {
    await ingestCategory(categoryName)
  }
  
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // RAPPORT FINAL
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const durationMinutes = ((Date.now() - stats.startTime) / 1000 / 60).toFixed(1)
  
  console.log('\n\n' + 'â•”' + 'â•'.repeat(58) + 'â•—')
  console.log('â•‘' + ' '.repeat(58) + 'â•‘')
  console.log('â•‘           âœ… INGESTION TERMINÃ‰E !                     â•‘')
  console.log('â•‘' + ' '.repeat(58) + 'â•‘')
  console.log('â•š' + 'â•'.repeat(58) + 'â•\n')
  
  console.log('ðŸ“ˆ RÃ©sultats:')
  console.log(`   âœ… Documents ingÃ©rÃ©s: ${stats.totalDocuments}`)
  console.log(`   âœ… Chunks crÃ©Ã©s: ${stats.totalChunks}`)
  console.log(`   âœ… CatÃ©gories: ${stats.categoriesProcessed.size}`)
  console.log(`   âŒ Erreurs: ${stats.totalErrors}`)
  console.log(`   â±ï¸  DurÃ©e totale: ${durationMinutes} minutes`)
  console.log(`   ðŸ“Š Moyenne: ${(stats.totalChunks / stats.totalDocuments).toFixed(0)} chunks/document`)
  
  if (stats.totalErrors > 0) {
    console.warn(`\nâš ï¸  ${stats.totalErrors} erreurs rencontrÃ©es. VÃ©rifiez les logs ci-dessus.`)
  } else {
    console.log('\nðŸŽ‰ Aucune erreur ! Tous les documents ont Ã©tÃ© ingÃ©rÃ©s avec succÃ¨s.')
  }
  
  console.log('\nðŸ’¡ Prochaine Ã©tape: Lancez `npm run ingest:verify` pour vÃ©rifier la base de donnÃ©es.\n')
}

/**
 * Main CLI
 */
async function main() {
  const args = process.argv.slice(2)
  
  if (args.length === 0) {
    // No args: ingest all
    await ingestAll()
  } else {
    // Category specified
    const categoryName = args[0]
    await ingestCategory(categoryName)
    
    const durationMinutes = ((Date.now() - stats.startTime) / 1000 / 60).toFixed(1)
    console.log(`\nâœ… CatÃ©gorie ${categoryName} ingÃ©rÃ©e en ${durationMinutes} minutes`)
    console.log(`   Documents: ${stats.totalDocuments}`)
    console.log(`   Chunks: ${stats.totalChunks}`)
    console.log(`   Erreurs: ${stats.totalErrors}`)
  }
}

// Run
main().catch(error => {
  console.error('\nðŸ’¥ ERREUR FATALE:', error)
  process.exit(1)
})
