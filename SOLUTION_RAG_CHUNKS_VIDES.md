# üîß SOLUTION: RAG Chunks Vides - Guide Complet

**Date:** 2026-01-29 15:50  
**Agent:** Amp  
**Statut:** ‚ö†Ô∏è BLOQU√â PAR R√âSEAU OFFLINE - Solution document√©e

---

## üö® Probl√®me Critique

**Sympt√¥me:** L'IA r√©pond syst√©matiquement "Puisque je n'ai aucun document √† disposition..."

**Cause racine:** Table `document_chunks` vide (0 rows) malgr√© 259 documents dans `documents`

```sql
SELECT COUNT(*) FROM documents;        -- 259 ‚úÖ
SELECT COUNT(*) FROM document_chunks;  -- 0   ‚ùå
```

**Cons√©quence:** Vector search retourne [] ‚Üí Gemini re√ßoit 0 contexte ‚Üí fallback internet 100%

---

## üìä √âtat Actuel

### Documents Existants
- **259 documents** ing√©r√©s dans table `documents`
- **Tous** ont `file_url` et `source_url` valides
- **Cat√©gories:** PAVILLON_MALTA (18), TVA_CHARTER_MED (22), PAVILLON_MARSHALL (12), CODES_REGS (30+), MYBA, AML_KYC, etc.
- **Aucun** n'a de champ `content` (juste m√©tadonn√©es + URLs)

### Storage Supabase
- **1 seul PDF** dans bucket `documents/documents/`
- **Les 259 autres** ne sont PAS stock√©s dans Supabase Storage
- **Implication:** Chunking n√©cessite re-t√©l√©chargement depuis URLs

### R√©seau
- **Status:** ‚ùå OFFLINE (getaddrinfo ENOTFOUND)
- **URLs test√©es:** www.yachtmca.com, autres ‚Üí FAIL
- **Blocage:** Impossible de t√©l√©charger PDFs/HTMLs

---

## ‚úÖ Solution Compl√®te

### Option 1: Ingestion Automatique (RECOMMAND√â)

**Quand:** Une fois le r√©seau disponible

**Script:** `scripts/ingest-reference-docs.ts` (D√âJ√Ä COMPLET)

**Fonctionnalit√©s:**
- ‚úÖ T√©l√©charge PDFs + scrape HTML
- ‚úÖ Extrait texte (pdf-parse + cheerio)
- ‚úÖ Chunking (500 tokens, 200 overlap, m√©tadonn√©es)
- ‚úÖ G√©n√®re embeddings (batch de 10, 768 dims)
- ‚úÖ Ins√®re dans `document_chunks`
- ‚úÖ Rate limiting + retry logic + stats

**Commande:**
```bash
cd /home/julien/Documents/iayacht/yacht-legal-ai
npm run ingest:all 2>&1 | tee logs/ingestion-full-$(date +%Y%m%d-%H%M%S).log
```

**Dur√©e estim√©e:** ~45-60 min (259 docs, rate limits OpenAI)

**R√©sultat attendu:**
- `document_chunks`: 3000-5000 chunks ins√©r√©s
- Embeddings: 768 dimensions chacun
- Avg: 12-20 chunks/document

### Option 2: Ingestion S√©lective (si Option 1 √©choue)

**Sc√©nario:** Certaines URLs sont cass√©es

**Script √† cr√©er:** `scripts/reingest-from-urls-robust.ts`

**Logique:**
```typescript
1. Fetch 259 documents from DB
2. For each document:
   - Try download from file_url (timeout 30s)
   - If fail, try source_url
   - If fail, skip and log error
   - If success:
     * Extract text
     * Chunk (500 tokens, 200 overlap)
     * Generate embeddings (batch 10)
     * Insert into document_chunks
3. Report:
   - Succ√®s: X/259
   - √âchecs: Y URLs (with reasons)
   - Chunks cr√©√©s: Z total
```

**Avantages:** Resilient aux URLs cass√©es

**Inconv√©nient:** Plus de code √† √©crire

---

## üöÄ √âtapes d'Ex√©cution (Pour Julien)

### Pr√©-requis

1. **V√©rifier r√©seau:** `ping google.com` ou `curl -I https://www.yachtmca.com`
2. **V√©rifier env:** `.env.local` contient `GEMINI_API_KEY`, `SUPABASE_SERVICE_ROLE_KEY`
3. **V√©rifier d√©pendances:** `npm install` (d√©j√† fait normalement)

### Ex√©cution

```bash
# 1. Aller dans le projet
cd ~/Documents/iayacht/yacht-legal-ai

# 2. Cr√©er dossier logs si n√©cessaire
mkdir -p logs

# 3. Lancer l'ingestion
npm run ingest:all 2>&1 | tee logs/ingestion-$(date +%Y%m%d-%H%M%S).log

# 4. Pendant l'ex√©cution, surveiller:
# - Progression (batch X/Y)
# - Erreurs (URLs cass√©es, rate limits)
# - Chunks cr√©√©s

# 5. En cas d'interruption:
# Le script est idempotent: il skip les docs d√©j√† chunk√©s
# Relancer simplement la m√™me commande
```

### Monitoring

**Terminal 1:**
```bash
# Suivre les logs en temps r√©el
tail -f logs/ingestion-<timestamp>.log
```

**Terminal 2:**
```bash
# V√©rifier progression DB (toutes les 30s)
watch -n 30 'curl -s -H "apikey: <SERVICE_ROLE_KEY>" "https://<PROJECT>.supabase.co/rest/v1/document_chunks?select=count" -H "Range-Unit: items" -H "Prefer: count=exact" | grep -o "content-range: [0-9]*/[0-9]*"'
```

### Post-V√©rification

```bash
# 1. Compter chunks
npm run db:count-chunks  # Script √† cr√©er ou SQL direct

# 2. Tester RAG
npm run test:e2e

# 3. Question test
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the requirements for Malta commercial yacht registration?"}'

# Attendu: R√©ponse avec 3+ citations du CYC, OGSR Malta, etc.
```

---

## üìã V√©rifications D√©taill√©es

### 1. Chunks Cr√©√©s

```sql
-- Total chunks
SELECT COUNT(*) as total_chunks FROM document_chunks;
-- Attendu: 3000-5000

-- Avg chunks par document
SELECT 
  COUNT(DISTINCT document_id) as total_docs,
  COUNT(*) as total_chunks,
  ROUND(COUNT(*)::numeric / COUNT(DISTINCT document_id), 2) as avg_chunks_per_doc
FROM document_chunks;
-- Attendu: 259 docs, 3000-5000 chunks, avg 12-20

-- Documents sans chunks
SELECT d.id, d.name, d.category
FROM documents d
LEFT JOIN document_chunks dc ON dc.document_id = d.id
WHERE dc.id IS NULL;
-- Attendu: 0 rows (ou quelques docs si URLs cass√©es)
```

### 2. Embeddings Valides

```sql
-- Dimension embeddings
SELECT 
  chunk_index,
  array_length(chunk_vector, 1) as embedding_dim
FROM document_chunks
LIMIT 5;
-- Attendu: embedding_dim = 768 pour tous

-- Chunks sans embeddings
SELECT COUNT(*) 
FROM document_chunks 
WHERE chunk_vector IS NULL 
   OR array_length(chunk_vector, 1) != 768;
-- Attendu: 0
```

### 3. Test RAG End-to-End

```typescript
// scripts/test-rag-after-ingestion.ts
import './load-env'
import { searchDocuments } from '../lib/search-documents'
import { generateAnswer } from '../lib/gemini'

async function main() {
  const question = "What are the requirements for Malta commercial yacht registration?"
  
  console.log(`Question: ${question}\n`)
  
  // Step 1: Search documents
  const results = await searchDocuments(question, {})
  console.log(`‚úÖ Retrieved ${results.length} chunks`)
  
  if (results.length === 0) {
    console.error('‚ùå FAIL: No chunks retrieved')
    process.exit(1)
  }
  
  // Step 2: Generate answer
  const context = results.map(r => r.chunk_text)
  const metadata = results.map(r => ({
    name: r.documentName,
    category: r.category,
    url: r.source_url
  }))
  
  const answer = await generateAnswer(question, context, [], metadata)
  
  console.log(`\n‚úÖ Answer:\n${answer}\n`)
  
  // Validation
  const hasCitations = answer.includes('[Source:')
  const mentionsMalta = answer.toLowerCase().includes('malta')
  
  if (!hasCitations) {
    console.error('‚ùå FAIL: No citations in answer')
    process.exit(1)
  }
  
  if (!mentionsMalta) {
    console.error('‚ö†Ô∏è  WARNING: Answer doesn\'t mention Malta')
  }
  
  console.log('‚úÖ TEST PASSED')
}

main()
```

---

## üéØ M√©triques de Succ√®s

| M√©trique | Avant | Objectif | Commande |
|----------|-------|----------|----------|
| **Documents** | 259 | 259 | `SELECT COUNT(*) FROM documents` |
| **Chunks** | 0 | 3000-5000 | `SELECT COUNT(*) FROM document_chunks` |
| **Avg chunks/doc** | 0 | 12-20 | `SELECT AVG(chunk_count) FROM ...` |
| **Embedding dim** | N/A | 768 | `SELECT array_length(chunk_vector, 1) ...` |
| **Search results** | 0 | 5-10 | Test RAG query |
| **Citations** | 0% | 80%+ | Test Gemini answers |
| **Fallback internet** | 100% | <20% | Monitor logs |

---

## ‚ö†Ô∏è Probl√®mes Potentiels & Solutions

### 1. Rate Limiting OpenAI

**Sympt√¥me:** Erreur 429 "Rate limit exceeded"

**Solution:**
```typescript
// Dans ingest-reference-docs.ts
const BATCH_SIZE = 5  // R√©duire de 10 ‚Üí 5
const DELAY_BETWEEN_BATCHES = 5000  // Augmenter de 2s ‚Üí 5s
```

### 2. URLs Cass√©es

**Sympt√¥me:** √âchec download pour certains documents

**Solution:**
- Noter les URLs cass√©es dans logs
- Cr√©er issue pour mise √† jour `reference-urls.ts`
- Chercher sources alternatives
- Re-run ingestion apr√®s mise √† jour

### 3. Embeddings Dimension Mismatch

**Sympt√¥me:** Erreur "dimension mismatch" lors de vector search

**V√©rification:**
```sql
SELECT DISTINCT array_length(chunk_vector, 1) as dim
FROM document_chunks;
-- Doit retourner UNE SEULE valeur: 768
```

**Solution:** Si plusieurs dimensions trouv√©es:
```sql
-- Supprimer chunks avec mauvaise dimension
DELETE FROM document_chunks
WHERE array_length(chunk_vector, 1) != 768;

-- Re-ing√©rer ces documents
```

### 4. Out of Memory

**Sympt√¥me:** Node crashes avec "JavaScript heap out of memory"

**Solution:**
```bash
# Augmenter heap size
NODE_OPTIONS="--max-old-space-size=4096" npm run ingest:all
```

---

## üõ†Ô∏è Scripts Utilitaires

### 1. Compter Chunks

```bash
cat > scripts/count-chunks.ts << 'EOF'
import './load-env'
import { supabaseAdmin } from '../lib/supabase'

async function main() {
  const { count, error } = await supabaseAdmin
    .from('document_chunks')
    .select('*', { count: 'exact', head: true })
  
  if (error) {
    console.error('Error:', error)
    process.exit(1)
  }
  
  console.log(`Total chunks: ${count}`)
}

main()
EOF

npm run count-chunks
```

### 2. Lister Documents Sans Chunks

```bash
cat > scripts/find-missing-chunks.ts << 'EOF'
import './load-env'
import { supabaseAdmin } from '../lib/supabase'

async function main() {
  const { data: docs, error: docsError } = await supabaseAdmin
    .from('documents')
    .select('id, name, category')
  
  if (docsError || !docs) {
    console.error('Error:', docsError)
    process.exit(1)
  }
  
  const missing = []
  
  for (const doc of docs) {
    const { count } = await supabaseAdmin
      .from('document_chunks')
      .select('*', { count: 'exact', head: true })
      .eq('document_id', doc.id)
    
    if (count === 0) {
      missing.push(doc)
    }
  }
  
  console.log(`Documents without chunks: ${missing.length}/${docs.length}`)
  missing.forEach(d => console.log(`  - [${d.category}] ${d.name}`))
}

main()
EOF

npm run find-missing
```

### 3. Supprimer Tous les Chunks (Reset)

```bash
cat > scripts/reset-chunks.ts << 'EOF'
import './load-env'
import { supabaseAdmin } from '../lib/supabase'

async function main() {
  console.log('‚ö†Ô∏è  WARNING: This will DELETE ALL chunks!')
  
  const { error } = await supabaseAdmin
    .from('document_chunks')
    .delete()
    .neq('id', '00000000-0000-0000-0000-000000000000')  // Delete all
  
  if (error) {
    console.error('Error:', error)
    process.exit(1)
  }
  
  console.log('‚úÖ All chunks deleted')
}

main()
EOF

# √Ä utiliser UNIQUEMENT si ingestion √©choue compl√®tement
npm run reset-chunks
```

---

## üìù Checklist Finale

### Avant Ingestion
- [ ] R√©seau op√©rationnel (test `curl` ou `ping`)
- [ ] `.env.local` complet (GEMINI_API_KEY, SUPABASE_SERVICE_ROLE_KEY)
- [ ] `npm install` √† jour
- [ ] Dossier `logs/` cr√©√©

### Pendant Ingestion
- [ ] Surveiller logs (erreurs, progression)
- [ ] V√©rifier DB chunks count augmente
- [ ] Noter URLs cass√©es si erreurs

### Apr√®s Ingestion
- [ ] V√©rifier chunk count (3000-5000)
- [ ] V√©rifier embedding dim (768)
- [ ] V√©rifier documents sans chunks (0)
- [ ] Tester RAG query (Malta, TVA, etc.)
- [ ] V√©rifier citations pr√©sentes (80%+)
- [ ] Tester E2E avec `npm run test:e2e`

### Si Succ√®s
- [ ] Commiter modifications
- [ ] Push vers repo
- [ ] Mettre √† jour CLAUDE.md avec r√©sultats
- [ ] Fermer ticket/issue Perplexity

---

## üéâ R√©sultat Attendu

**AVANT:**
```
User: "Malta commercial yacht requirements?"
AI: "Puisque je n'ai aucun document √† disposition..."
```

**APR√àS:**
```
User: "Malta commercial yacht requirements?"
AI: "Pour enregistrer un yacht commercial √† Malte, voici les principales exigences:

1. √âligibilit√© propri√©taire: Selon l'OGSR Malta, les propri√©taires doivent... [Source: OGSR Malta Yacht Code, page 12]

2. Conformit√© CYC 2020/2025: Le yacht doit satisfaire... [Source: CYC Code Complete 2020 Edition, section 3.2]

3. Inspections et surveys: Pour un yacht de 38m construit en 2010... [Source: Transport Malta Registration Process]

(3+ citations minimum, contexte pr√©cis, 0% fallback internet)
```

---

**üöÄ PR√äT √Ä EX√âCUTER**

**Commande:**
```bash
cd ~/Documents/iayacht/yacht-legal-ai
npm run ingest:all 2>&1 | tee logs/ingestion-$(date +%Y%m%d-%H%M%S).log
```

**Note pour Julien:** Une fois le r√©seau op√©rationnel, lance cette commande et laisse tourner ~1h. Amp ne peut pas l'ex√©cuter maintenant (r√©seau offline).

---

**G√©n√©r√© par:** Amp  
**Date:** 2026-01-29 15:50  
**Status:** ‚è≥ EN ATTENTE R√âSEAU
