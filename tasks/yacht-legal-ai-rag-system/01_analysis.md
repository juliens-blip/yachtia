# Analyse: Yacht Legal AI - SystÃ¨me RAG Complet

## ğŸ“‹ Contexte
**Date:** 2026-01-13  
**Demande initiale:** AmÃ©liorer le systÃ¨me RAG existant avec:
- Ingestion automatique des 70+ documents de rÃ©fÃ©rence (MYBA, AML, MLC, YET, etc.)
- Refonte de l'interface chat style GPT/Gemini
- IntÃ©gration Gemini Grounding (recherche web temps rÃ©el)
- API REST pour agents MCP externes

**Objectif:** SystÃ¨me juridique maritime complet et production-ready

---

## ğŸ” Ã‰tat Actuel de la Codebase

### Fichiers ConcernÃ©s

| Fichier | Type | RÃ´le | Lignes |
|---------|------|------|--------|
| `lib/gemini.ts` | Utility | GÃ©nÃ¨re embeddings (768 dim) + rÃ©ponses Gemini | 104 |
| `lib/rag-pipeline.ts` | Utility | Orchestration RAG (retrieve + format + stats) | 166 |
| `lib/supabase.ts` | Utility | Clients Supabase (admin + browser) | ~50 |
| `lib/chunker.ts` | Utility | Chunking intelligent (500 tokens, 100 overlap) | ~100 |
| `lib/pdf-parser.ts` | Utility | Extraction texte PDF | ~80 |
| `lib/audit-logger.ts` | Utility | Logs RGPD (chat + upload) | ~60 |
| `app/api/chat/route.ts` | API Route | Endpoint chat avec RAG complet | 175 |
| `app/api/upload-doc/route.ts` | API Route | Upload PDF + embedding | ~200 |
| `app/api/search/route.ts` | API Route | Recherche vectorielle pure | ~80 |
| `components/ChatInterface.tsx` | Component | Interface chat (basique) | 158 |
| `components/DocumentUploader.tsx` | Component | Upload PDF avec feedback | ~150 |
| `database/migrations/*.sql` | SQL | 7 migrations (pgvector + tables + RLS) | ~500 |
| `package.json` | Config | DÃ©pendances (Gemini, Supabase, Next.js 14) | 34 |

**Total:** ~1700 lignes de code backend/frontend dÃ©jÃ  fonctionnel

---

### Architecture Actuelle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUX RAG ACTUEL                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚  "Quelles sont les obligations AML en France?"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /api/chat (POST)    â”‚  Rate limiting (10/min in-memory)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generateEmbedding() â”‚  Gemini text-embedding-004 (768 dim)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ retrieveRelevantChunks() â”‚  pgvector search (cosine similarity)
â”‚                      â”‚  Parameters: topK=5, threshold=0.7
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Supabase RPC        â”‚  search_documents(query_embedding, ...)
â”‚ IVFFlat Index       â”‚  Returns top 5 chunks with similarity
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ formatChunksForContext() â”‚  Ajoute mÃ©tadonnÃ©es sources
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generateAnswer()    â”‚  Gemini 2.0 Flash
â”‚                     â”‚  Prompt: system + context + question
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Store Conversation  â”‚  Table 'conversations' (messages JSONB)
â”‚ Audit Log           â”‚  Table 'audit_logs' (RGPD tracking)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return JSON         â”‚  { answer, conversationId, sources[] }
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatInterface.tsx   â”‚  Affiche rÃ©ponse + sources (basique)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Code Snippets ClÃ©s

#### 1. GÃ©nÃ©ration d'Embeddings (lib/gemini.ts:25-39)
```typescript
export async function generateEmbedding(text: string): Promise<number[]> {
  const model = genAI.getGenerativeModel({ model: 'text-embedding-004' })
  const result = await model.embedContent(text)
  
  if (!result.embedding || !result.embedding.values) {
    throw new Error('No embedding returned from Gemini API')
  }
  
  return result.embedding.values  // 768 dimensions
}
```

**Points clÃ©s:**
- ModÃ¨le: `text-embedding-004` (derniÃ¨re version Gemini)
- Dimension: 768 (fixe, compatible pgvector)
- Usage: Query + chunks de documents

#### 2. Recherche Vectorielle (lib/rag-pipeline.ts:45-86)
```typescript
export async function retrieveRelevantChunks(
  query: string,
  category?: string,
  topK: number = 5,
  similarityThreshold: number = 0.7
): Promise<RelevantChunk[]> {
  // Ã‰tape 1: GÃ©nÃ©rer embedding de la query
  const queryEmbedding = await generateEmbedding(query)
  
  // Ã‰tape 2: Recherche pgvector via RPC
  const { data, error } = await supabaseAdmin
    .rpc('search_documents', {
      query_embedding: queryEmbedding,
      match_threshold: similarityThreshold,
      match_count: topK,
      filter_category: category || null
    })
  
  // Ã‰tape 3: Formater rÃ©sultats
  const chunks: RelevantChunk[] = (data || []).map((row) => ({
    chunkId: row.chunk_id,
    documentName: row.document_name,
    category: row.category,
    chunkText: row.chunk_text,
    similarity: row.similarity,
    pageNumber: row.page_number,
    chunkIndex: row.chunk_index
  }))
  
  return chunks
}
```

**Points clÃ©s:**
- Seuil: 0.7 (70% similaritÃ© minimum)
- Top-K: 5 chunks par dÃ©faut
- Filtrage par catÃ©gorie (optionnel)
- Index IVFFlat (lists=100) pour performance

#### 3. Chat API avec RAG (app/api/chat/route.ts:37-150)
```typescript
export async function POST(req: NextRequest) {
  const startTime = Date.now()
  
  // Rate limiting (10 req/min)
  const ip = req.headers.get('x-forwarded-for') || 'unknown'
  if (!checkRateLimit(ip)) {
    return NextResponse.json(
      { error: 'Rate limit exceeded. Maximum 10 requests per minute.' },
      { status: 429 }
    )
  }
  
  const { message, conversationId, category } = await req.json()
  
  // Validation
  if (!message || message.length > 2000) {
    return NextResponse.json({ error: 'Invalid message' }, { status: 400 })
  }
  
  // RAG pipeline
  const chunks = await retrieveRelevantChunks(message, category, 5, 0.7)
  const context = formatChunksForContext(chunks)
  const answer = await generateAnswer(message, context)
  
  // Store conversation
  let convId = conversationId
  if (!convId) {
    const { data } = await supabaseAdmin
      .from('conversations')
      .insert({ messages: [...], document_ids: [...] })
      .select('id')
      .single()
    convId = data.id
  }
  
  // Audit log
  await logChatAudit({ conversationId: convId, query: message, ... })
  
  // Return
  return NextResponse.json({
    answer,
    conversationId: convId,
    sources: chunks.map(c => ({ documentName, category, similarity })),
    responseTime: Date.now() - startTime
  })
}
```

**Points clÃ©s:**
- Rate limiting in-memory (simple Map)
- Validation stricte (longueur, type)
- Tracking response time
- Audit RGPD automatique

#### 4. Interface Chat (components/ChatInterface.tsx:12-158)
```typescript
export default function ChatInterface() {
  const [messages, setMessages] = useState<Message[]>([])
  const [input, setInput] = useState('')
  const [loading, setLoading] = useState(false)
  const [conversationId, setConversationId] = useState<string | null>(null)
  
  const handleSend = async () => {
    if (!input.trim() || loading) return
    
    // Ajouter message utilisateur
    setMessages(prev => [...prev, {
      role: 'user',
      content: input.trim(),
      timestamp: new Date().toISOString()
    }])
    
    setLoading(true)
    
    // Appel API
    const response = await fetch('/api/chat', {
      method: 'POST',
      body: JSON.stringify({ message: input, conversationId })
    })
    
    const data = await response.json()
    
    // Ajouter rÃ©ponse assistant
    setMessages(prev => [...prev, {
      role: 'assistant',
      content: data.answer,
      sources: data.sources,
      timestamp: new Date().toISOString()
    }])
    
    setLoading(false)
  }
  
  return (
    <div className="flex flex-col h-full">
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-6 space-y-4">
        {messages.map((msg, i) => <MessageBubble key={i} {...msg} />)}
      </div>
      
      {/* Input */}
      <div className="border-t p-4">
        <textarea
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && !e.shiftKey && handleSend()}
          placeholder="Posez votre question..."
        />
        <button onClick={handleSend} disabled={loading}>
          {loading ? 'Envoi...' : 'Envoyer'}
        </button>
      </div>
    </div>
  )
}
```

**Points clÃ©s:**
- Ã‰tat local (messages, loading)
- Auto-scroll vers le bas
- Raccourci Enter (sans Shift)
- Gestion erreurs basique

---

## ğŸ“š DÃ©pendances Externes

### NPM Packages (package.json)
```json
{
  "dependencies": {
    "next": "14.2.35",              // Framework (App Router)
    "react": "^18",                  // UI library
    "react-dom": "^18",              // DOM rendering
    "@supabase/supabase-js": "^2.38.0",  // DB client
    "@google/generative-ai": "^0.11.0",  // Gemini API
    "pdf-parse": "^1.1.1",           // Extraction PDF
    "uuid": "^9.0.0",                // ID generation
    "zustand": "^4.4.7"              // State mgmt (non utilisÃ©)
  },
  "devDependencies": {
    "typescript": "^5",
    "@types/node": "^20",
    "@types/react": "^18",
    "tailwindcss": "^3.4.1",         // Styling
    "eslint": "^8",
    "eslint-config-next": "14.2.35"
  }
}
```

### Services Externes
1. **Gemini AI API** (Google)
   - ModÃ¨le embeddings: `text-embedding-004` (768 dim)
   - ModÃ¨le chat: `gemini-2.0-flash` (rapide + gratuit)
   - ClÃ© API: `GEMINI_API_KEY` (env variable)

2. **Supabase** (PostgreSQL + pgvector)
   - URL: `SUPABASE_URL`
   - Anon Key: `SUPABASE_ANON_KEY`
   - Service Key: `SUPABASE_SERVICE_KEY` (admin operations)
   - Tables: `documents`, `document_chunks`, `conversations`, `audit_logs`
   - Extension: `pgvector` (vector similarity search)

---

## ğŸ—„ï¸ SchÃ©ma Base de DonnÃ©es

### Tables (7 migrations SQL)

#### 1. `documents`
```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT NOT NULL,
  category TEXT NOT NULL,  -- MYBA, AML, MLC, YET, etc.
  pages INTEGER,
  upload_date TIMESTAMPTZ DEFAULT NOW(),
  file_url TEXT,
  metadata JSONB
);
```
**RÃ´le:** MÃ©tadonnÃ©es des PDFs uploadÃ©s

#### 2. `document_chunks`
```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index INTEGER NOT NULL,
  chunk_text TEXT NOT NULL,
  embedding VECTOR(768),  -- pgvector extension
  page_number INTEGER,
  token_count INTEGER,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index pour recherche vectorielle
CREATE INDEX idx_chunks_embedding ON document_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```
**RÃ´le:** Chunks de texte + embeddings pour RAG

#### 3. `conversations`
```sql
CREATE TABLE conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  messages JSONB NOT NULL,  -- [{role, content, timestamp}]
  document_ids TEXT[],      -- IDs docs utilisÃ©s
  title TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  last_message_at TIMESTAMPTZ DEFAULT NOW()
);
```
**RÃ´le:** Historique conversations utilisateur

#### 4. `audit_logs`
```sql
CREATE TABLE audit_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  action_type TEXT NOT NULL,  -- 'chat_query', 'document_upload'
  conversation_id UUID,
  user_ip TEXT,
  user_agent TEXT,
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Nettoyage auto aprÃ¨s 2 ans (RGPD)
```
**RÃ´le:** Logs RGPD pour conformitÃ©

---

## ğŸ”— Connexions entre Fichiers

### Flow d'un Chat Query

```
User Browser (ChatInterface.tsx)
    â†“ POST /api/chat { message: "..." }
app/api/chat/route.ts
    â†“ retrieveRelevantChunks(message)
lib/rag-pipeline.ts
    â†“ generateEmbedding(query)
lib/gemini.ts
    â†“ Gemini API (text-embedding-004)
    â† [768 numbers]
lib/rag-pipeline.ts
    â†“ supabaseAdmin.rpc('search_documents', ...)
lib/supabase.ts
    â†“ PostgreSQL + pgvector
database/migrations/006_create_search_function.sql
    â† Top 5 chunks (similarity > 0.7)
lib/rag-pipeline.ts
    â†“ formatChunksForContext(chunks)
    â†’ ["[Document: X]\nChunk text...", ...]
app/api/chat/route.ts
    â†“ generateAnswer(message, context)
lib/gemini.ts
    â†“ Gemini API (gemini-2.0-flash)
    â† "RÃ©ponse avec citations..."
app/api/chat/route.ts
    â†“ logChatAudit({ conversationId, query, ... })
lib/audit-logger.ts
    â†“ supabaseAdmin.from('audit_logs').insert(...)
    â†“ Store conversation in DB
    â†’ Response JSON { answer, sources, conversationId }
User Browser (ChatInterface.tsx)
    â† Displays answer + sources
```

### Flow d'un Upload PDF

```
User Browser (DocumentUploader.tsx)
    â†“ POST /api/upload-doc { file: Blob, category: "MYBA" }
app/api/upload-doc/route.ts
    â†“ Validate file (type, size)
    â†“ Upload to Supabase Storage
lib/supabase.ts (storage bucket)
    â† file_url
app/api/upload-doc/route.ts
    â†“ parsePDF(fileBuffer)
lib/pdf-parser.ts
    â† { text: "...", pages: 20 }
app/api/upload-doc/route.ts
    â†“ chunkText(text, 500, 100)
lib/chunker.ts
    â† ["chunk1", "chunk2", ...]
app/api/upload-doc/route.ts
    â†“ for each chunk: generateEmbedding(chunk)
lib/gemini.ts
    â† [768 numbers] Ã— N chunks
app/api/upload-doc/route.ts
    â†“ supabaseAdmin.from('documents').insert({ name, category, ... })
    â†“ supabaseAdmin.from('document_chunks').insert([{ chunk_text, embedding }, ...])
lib/supabase.ts
    â†’ Success response { documentId, chunks: N, pages: 20 }
User Browser (DocumentUploader.tsx)
    â† "Document uploadÃ© avec succÃ¨s !"
```

---

## âš ï¸ Points d'Attention

### 1. **Tables vides** âŒ
**ProblÃ¨me:** `documents` et `document_chunks` sont vides  
**Impact:** Le chat rÃ©pond "Je n'ai pas trouvÃ© d'information..." car aucune donnÃ©e de rÃ©fÃ©rence  
**Solution:** CrÃ©er script d'ingestion automatique pour les 70+ URLs

### 2. **Pas de streaming** â³
**ProblÃ¨me:** `/api/chat` retourne rÃ©ponse complÃ¨te d'un coup  
**Impact:** Utilisateur attend 2-3 sec sans feedback (mauvaise UX)  
**Solution:** ImplÃ©menter streaming avec `ReadableStream` + Gemini `generateContentStream()`

### 3. **Interface basique** ğŸ¨
**ProblÃ¨me:** `ChatInterface.tsx` est fonctionnel mais pas style GPT/Gemini  
**Manque:**
- Pas de sidebar conversations
- Pas de bouton "Nouvelle conversation"
- Pas de markdown rendering (code blocks, listes)
- Pas de citations cliquables
- Design simple (pas dark mode)

### 4. **Rate limiting in-memory** ğŸ’¾
**ProblÃ¨me:** Map JavaScript reset au redÃ©marrage serveur  
**Impact:** Production multi-instance ne fonctionne pas  
**Solution:** Migrer vers Redis (Upstash) ou Vercel KV

### 5. **Pas de Gemini Grounding** ğŸŒ
**ProblÃ¨me:** SystÃ¨me rÃ©pond UNIQUEMENT avec docs Supabase  
**Manque:** Recherche web temps rÃ©el pour infos rÃ©centes  
**Solution:** Ajouter Google Search grounding dans Gemini

### 6. **Pas d'API pour agents** ğŸ¤–
**ProblÃ¨me:** Agents MCP externes ne peuvent pas interroger le systÃ¨me  
**Solution:** CrÃ©er endpoints REST dÃ©diÃ©s (`/api/agents/query`, `/api/agents/search`)

### 7. **Token counting approximatif** ğŸ“
**ProblÃ¨me:** `chunker.ts` utilise `text.split(/\s+/).length` au lieu de tokenizer  
**Impact:** Chunks peuvent dÃ©passer 500 tokens rÃ©els  
**Solution:** Utiliser `js-tiktoken` (encodage OpenAI) ou Gemini tokenizer

---

## ğŸ’¡ OpportunitÃ©s IdentifiÃ©es

### 1. **Script d'ingestion automatique** ğŸš€
**OpportunitÃ©:** Automatiser l'upload des 70+ documents de rÃ©fÃ©rence  
**BÃ©nÃ©fice:** SystÃ¨me opÃ©rationnel en 10 minutes au lieu de 2 heures manuelles  
**ImplÃ©mentation:**
```typescript
// scripts/ingest-reference-docs.ts
const REFERENCE_URLS = {
  MYBA: ['https://...', ...],
  AML: ['https://...', ...],
  // ...
}

async function ingestAll() {
  for (const [category, urls] of Object.entries(REFERENCE_URLS)) {
    for (const url of urls) {
      await downloadAndIngest(url, category)
    }
  }
}
```

### 2. **Interface GPT-style** âœ¨
**OpportunitÃ©:** AmÃ©liorer drastiquement l'UX du chat  
**Features Ã  ajouter:**
- Sidebar avec historique conversations
- Markdown rendering avec `react-markdown`
- Citations cliquables (scroll to source)
- Dark mode (theme Tailwind)
- Auto-resize textarea
- Streaming des tokens

### 3. **Gemini Grounding** ğŸ”
**OpportunitÃ©:** Combiner docs de rÃ©fÃ©rence + recherche web  
**BÃ©nÃ©fice:** RÃ©ponses Ã  jour (nouvelles lois, jurisprudence rÃ©cente)  
**ImplÃ©mentation:**
```typescript
const model = genAI.getGenerativeModel({
  model: "gemini-2.0-flash",
  tools: [{
    googleSearch: {}  // Active grounding
  }]
})
```

### 4. **API REST pour agents** ğŸ”Œ
**OpportunitÃ©:** Exposer le RAG aux agents MCP externes  
**Use cases:**
- Agent MYBA Compliance vÃ©rifie contrats
- Agent AML check documents brokers
- Agent MLC analyse contrats crew

**Endpoints:**
```typescript
POST /api/agents/query
POST /api/agents/search
POST /api/agents/analyze-document
```

### 5. **Caching intelligent** âš¡
**OpportunitÃ©:** RÃ©duire latence et coÃ»ts API  
**StratÃ©gie:**
- Cache embeddings frÃ©quents (Redis)
- Cache rÃ©sultats RAG (TTL 1h)
- Cache rÃ©ponses Gemini (dedupe queries identiques)

### 6. **Monitoring & Analytics** ğŸ“Š
**OpportunitÃ©:** Tracker performance et usage  
**MÃ©triques:**
- Queries/jour par catÃ©gorie
- Temps de rÃ©ponse moyen
- Taux de satisfaction (sources utilisÃ©es ?)
- Top questions frÃ©quentes

---

## ğŸ“Š RÃ©sumÃ© ExÃ©cutif

### Ã‰tat Actuel
âœ… **Backend RAG fonctionnel** (embeddings, search, answer generation)  
âœ… **Base de donnÃ©es prÃªte** (pgvector, migrations, RLS)  
âœ… **API routes solides** (chat, upload, search, audit)  
âœ… **UI basique** (chat + upload opÃ©rationnels)  
âŒ **Documents vides** (aucune donnÃ©e de rÃ©fÃ©rence)  
âŒ **Interface simpliste** (pas style GPT/Gemini)  
âŒ **Pas de grounding** (recherche web absente)  
âŒ **Pas d'API agents** (pas d'intÃ©gration MCP)

### Prochaines Actions Critiques
1. **PRIORITÃ‰ 1:** Script d'ingestion des 70+ documents (rÃ©sout le problÃ¨me "pas de donnÃ©es")
2. **PRIORITÃ‰ 2:** Refonte UI chat (expÃ©rience utilisateur GPT-level)
3. **PRIORITÃ‰ 3:** Gemini Grounding (recherche web temps rÃ©el)
4. **PRIORITÃ‰ 4:** API pour agents MCP (intÃ©gration externe)

### ComplexitÃ© EstimÃ©e
- **Ingestion docs:** 4 heures (tÃ©lÃ©chargement + chunking + embeddings)
- **UI chat GPT-style:** 6 heures (sidebar, markdown, streaming, dark mode)
- **Gemini Grounding:** 2 heures (simple activation API)
- **API agents:** 4 heures (3 endpoints + documentation)

**Total:** ~16 heures de dÃ©veloppement

### Risques Techniques
âš ï¸ **Rate limits Gemini:** 70+ docs = ~1000 chunks = 1000 embeddings API calls  
   â†’ Solution: Batch processing avec delays (10 chunks/sec max)

âš ï¸ **Taille base Supabase:** 70 docs Ã— 20 pages Ã— 5 chunks/page = 7000 chunks Ã— 768 dim  
   â†’ Solution: VÃ©rifier quota Supabase (gratuit = 500 MB, OK pour ~100k chunks)

âš ï¸ **Streaming complexe:** Next.js API Route streaming nÃ©cessite ReadableStream  
   â†’ Solution: Utiliser `TransformStream` + `response.body.pipeThrough()`

---

## ğŸ¯ Recommandations StratÃ©giques

### Architecture ProposÃ©e (aprÃ¨s amÃ©liorations)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              YACHT LEGAL AI V2                         â”‚
â”‚                                                        â”‚
â”‚  User Query                                            â”‚
â”‚     â†“                                                  â”‚
â”‚  [ChatInterface GPT-style]                             â”‚
â”‚     â†“                                                  â”‚
â”‚  /api/chat (streaming)                                 â”‚
â”‚     â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ RAG Pipeline (Supabase Vector DB)   â”‚              â”‚
â”‚  â”‚ + Gemini Grounding (Web Search)     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚     â†“                                                  â”‚
â”‚  Fusion des contextes                                  â”‚
â”‚     â†“                                                  â”‚
â”‚  Gemini 2.0 Flash (+ streaming)                        â”‚
â”‚     â†“                                                  â”‚
â”‚  Response (markdown + sources cliquables)              â”‚
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ API Endpoints pour Agents MCP       â”‚              â”‚
â”‚  â”‚ - /api/agents/query                 â”‚              â”‚
â”‚  â”‚ - /api/agents/search                â”‚              â”‚
â”‚  â”‚ - /api/agents/analyze-document      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technologies Ã  Ajouter
1. **react-markdown** + **remark-gfm**: Rendering markdown (code blocks, tables)
2. **cheerio** + **node-fetch**: Scraping pages HTML pour ingestion
3. **p-queue**: Gestion batch embeddings (rate limiting)
4. **redis** (Upstash): Rate limiting distribuÃ©
5. **sentry**: Monitoring erreurs production

### DÃ©cisions Techniques
âœ… **Garder Next.js 14** (App Router stable et performant)  
âœ… **Garder Gemini 2.0 Flash** (gratuit, rapide, grounding intÃ©grÃ©)  
âœ… **Garder Supabase pgvector** (cosine similarity excellent)  
âœ… **Migrer vers streaming** (meilleure UX)  
âœ… **Ajouter Redis** (rate limiting production-ready)

---

**Date d'Analyse:** 2026-01-13  
**AnalysÃ© par:** Agent explore-code + backend-architect  
**Prochaine Ã‰tape:** CrÃ©er `02_plan.md` avec plan d'implÃ©mentation dÃ©taillÃ©

---

**Fichiers ClÃ©s IdentifiÃ©s:**
- âœ… Backend solide: `lib/gemini.ts`, `lib/rag-pipeline.ts`, `app/api/chat/route.ts`
- âœ… DB prÃªte: `database/migrations/006_create_search_function.sql`
- âš ï¸ UI Ã  refondre: `components/ChatInterface.tsx`
- âŒ Script ingestion: **Ã€ CRÃ‰ER** (`scripts/ingest-reference-docs.ts`)
- âŒ API agents: **Ã€ CRÃ‰ER** (`app/api/agents/*`)

**Confiance:** 95% (codebase bien structurÃ©, patterns clairs, documentation exhaustive)
